# TorchTransformer ~50M parameters (52.0 M)
# Purpose: Medium-scale training on TinyStories

experiment_name: "torch_50m"

model:
  d_model: 640
  n_blocks: 10
  n_heads: 10        # d_head = 64
  d_ffn: 2560        # 4x d_model
  max_seq_len: 256
  dtype: bfloat16

data:
  dataset: "tinystories"
  tokenizer: "tinystories_bpe_4096"
  max_length: 256
  val_subset_size: 1000

training:
  batch_size: 16
  gradient_accumulation_steps: 2  # effective batch size = 32
  learning_rate: 0.0003
  min_learning_rate: 0.00003
  lr_decay: cosine
  warmup_ratio: 0.05
  weight_decay: 0.01
  num_epochs: 3
  checkpoint_interval_minutes: 15

  log_every: 50
  eval_every: 500

  max_grad_norm: 1.0
