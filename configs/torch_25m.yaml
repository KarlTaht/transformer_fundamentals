# TorchTransformer ~25M parameters (24.3 M)
# Purpose: Small-scale training on TinyStories

experiment_name: "torch_25m"

model:
  d_model: 512
  n_blocks: 7
  n_heads: 8         # d_head = 64
  d_ffn: 2048        # 4x d_model
  max_seq_len: 256
  dtype: bfloat16

data:
  dataset: "tinystories"
  tokenizer: "tinystories_bpe_4096"
  max_length: 256
  val_subset_size: 1000

training:
  batch_size: 32
  gradient_accumulation_steps: 1
  learning_rate: 0.0003
  min_learning_rate: 0.00003
  lr_decay: cosine
  warmup_ratio: 0.05
  weight_decay: 0.01
  num_epochs: 3
  checkpoint_interval_minutes: 15

  log_every: 50
  eval_every: 500

  max_grad_norm: 1.0
